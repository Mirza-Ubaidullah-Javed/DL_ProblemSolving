{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\softwares\\anaconda\\lib\\site-packages (9.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (d:\\softwares\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\softwares\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\softwares\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\softwares\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\softwares\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\softwares\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Used to open and process GIF images\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This imports the NumPy library and aliases it as \"np.\" NumPy is a fundamental package for numerical computing in Python, \n",
    "and it provides support for arrays, matrices, and mathematical functions.'''\n",
    "import numpy as np\n",
    "\n",
    "'''This imports the TensorFlow library and aliases it as \"tf.\" \n",
    "TensorFlow is an open-source machine learning framework developed by Google for various machine learning and deep learning tasks.'''\n",
    "import tensorflow as tf\n",
    "\n",
    "'''his imports the \"Sequential\" class from the \"tensorflow.keras.models\" module. \n",
    "In TensorFlow, the \"Sequential\" class is commonly used to create sequential neural network models, where you stack layers sequentially.'''\n",
    "from keras.models import Sequential\n",
    "\n",
    "'''This imports the \"Dense\" and \"Flatten\" layer classes from the \"tensorflow.keras.layers\" module. These layers are building blocks \n",
    "for creating neural network architectures.\"Dense\" represents a fully connected layer, and \"Flatten\" is used to flatten the input data.'''\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "'''This imports the Stochastic Gradient Descent (SGD) optimizer from the \"tensorflow.keras.optimizers\" module. \n",
    "SGD is a popular optimization algorithm used for training neural networks.'''\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "'''This imports the \"GridSearchCV\" class from the \"sklearn.model_selection\" module. \n",
    "It's part of the Scikit-learn library and is used for hyperparameter tuning through grid search.'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "''' This imports various evaluation metrics and the confusion matrix function from the \"sklearn.metrics\" module. \n",
    "These metrics are commonly used to assess the performance of machine learning models, including accuracy, precision, recall, and F1-score.'''\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "''' This imports the Matplotlib library and aliases it as \"plt.\" \n",
    "Matplotlib is a popular library for creating visualizations, including plots and charts.'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''To read the GIF images'''\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\"\"\"This is needed to split the data into train, test, and validation set\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi layer perceptron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to implement a multi layer perceptron for facial recognition\n",
    "class MLP:\n",
    "    \n",
    "    '''Constructor to get important problem information:\n",
    "    1) num_classes: The number of output classes\n",
    "    2) image_width: The number of pixels in the horizontal direction\n",
    "    3) image_height: The number of pixels in the vertical direction\n",
    "    4) num_channels: The number of channels in the image'''\n",
    "    def __init__(self, num_classes, image_width, image_height, num_channels):\n",
    "        self.num_classes = num_classes\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.num_channels = num_channels\n",
    "    \n",
    "\n",
    "    '''Function to create the model. It has the following parameters\n",
    "    1) neurons_per_layer: The number of neurons (units) in each hidden layer of the neural network. The default is set to 64.\n",
    "    2) num_hidden_layer: The number of hidden layers in the neural network. The default is set to 2.\n",
    "    3) activation:  The activation function used in the hidden layers. The default is set to 'relu' (Rectified Linear Unit).\n",
    "    4) learning_rate: : The learning rate for the stochastic gradient descent (SGD) optimizer, which controls the step size during training. The default is set to 0.01.\n",
    "    5) momentum: The momentum term for SGD, which helps accelerate training. The default is set to 0.9.'''\n",
    "    def create_model(self, neurons_per_layer=64, num_hidden_layers=2, activation='relu', learning_rate=0.01, momentum=0.9):\n",
    "        # Create an instance of a sequential neural network model of the TensorFlow's Sequential class which allows to build neural network by stacking layers sequentially \n",
    "        model = Sequential()\n",
    "\n",
    "        # Add a flattened input layer used to flatter the input data. The input shape parameter specifies the shape of the input data(image width,height, and number of channels)\n",
    "        model.add(Flatten(input_shape=(self.image_width, self.image_height, self.num_channels)))\n",
    "\n",
    "        # Loop to add multiple dense(fully connected) hidden layers to the network\n",
    "        for _ in range(num_hidden_layers):\n",
    "            model.add(Dense(neurons_per_layer, activation=activation))\n",
    "\n",
    "        # Add an output dense layer having neurons equal to number of classes. The activation function is 'softmax,' which is common for multiclass classification problems.\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # An instance of the stochastic gradient descent (SGD) optimizer is created with the specified learning_rate and momentum parameters.\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Compile the model specifying the optimizer, loss function('categorical_crossentropy' for multiclass classification) and the metrics to track during training e.g accuracy\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # return the compiler neural network model\n",
    "        return model\n",
    "\n",
    "    '''Function to train the model. It has the following parameters:\n",
    "    1) X_train: The input to the training data\n",
    "    2) Y_train: The output to the training data\n",
    "    3) epochs: Specifies the number of training epochs, which is the number of times the model will be trained on the entire training dataset.\n",
    "    4) batch_size: Indicates how many samples from the training dataset are used in each iteration of training'''\n",
    "    def fit(self, X_train, y_train, epochs=10, batch_size=32):\n",
    "\n",
    "        # Create an instance of a neural network model\n",
    "        model = self.create_model()\n",
    "\n",
    "        # Start the training process. Verbose = 1 means the training progress will be showed\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "        # Return the trained model and its history(information)\n",
    "        return model, history\n",
    "\n",
    "    ''' Function to test the model's perfomance on unseen data. It has the following parameters:\n",
    "    1) model: The model to test\n",
    "    2) X_test: The input to the test data\n",
    "    3) y_test: The output to the test data'''\n",
    "    def test(self, model, X_test, y_test):\n",
    "\n",
    "        # Use the trained model to make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # \n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "        return accuracy\n",
    "\n",
    "    '''Function to tune the model'''\n",
    "    def tune(self, X_train, y_train, param_grid, cv=3):\n",
    "        model = self.create_model()\n",
    "        keras_classifier = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=lambda: model, verbose=0)\n",
    "        grid = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        grid_result = grid.fit(X_train, y_train)\n",
    "        return grid_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read GIF images from a directory\n",
    "def read_gif_images(directory):\n",
    "\n",
    "    # Store the image data\n",
    "    image_data = []\n",
    "\n",
    "    # Store image name\n",
    "    file_names = []\n",
    "\n",
    "    # Iterate through the files\n",
    "    for file in os.listdir(directory):\n",
    "    \n",
    "        # If the file is a GIF image, the code constructs the full file path by joining the specified directory (directory) and the current file name (file) using os.path.join\n",
    "        file_path = os.path.join(directory, file)\n",
    "\n",
    "        # Append the filename to the file_name list\n",
    "        file_names.append(file)\n",
    "\n",
    "        # Open the GIF image\n",
    "        image = Image.open(file_path)\n",
    "\n",
    "        # Convert the images to grayscale(They are already black and white)\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Append the opened image to image_data list\n",
    "        image_data.append(image)\n",
    "\n",
    "    # Return the GIF images, and there file names\n",
    "    return image_data, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "image_data, file_names = read_gif_images(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming image_data contains PIL Image objects\n",
    "def display_images(image_data, file_names):\n",
    "    num_images = len(image_data)\n",
    "\n",
    "    # Determine the number of rows and columns for subplots\n",
    "    rows = int(num_images / 4)  # Adjust the number of columns as needed\n",
    "    if num_images % 4 != 0:\n",
    "        rows += 1\n",
    "\n",
    "    # Create subplots for displaying images\n",
    "    fig, axes = plt.subplots(rows, 4, figsize=(12, 3 * rows))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_images:\n",
    "            ax.imshow(image_data[i])\n",
    "            ax.set_title(file_names[i])\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the display_images function\n",
    "display_images(image_data, file_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 115\n",
      "Number of samples in validation set: 25\n",
      "Number of samples in test set: 25\n"
     ]
    }
   ],
   "source": [
    "# Determine split ratios\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    image_data, file_names, test_size=(1 - train_ratio), random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio), random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Extract only the subject information from the file names\n",
    "y_train = [file_name.split('.')[0] for file_name in y_train]\n",
    "y_validation = [file_name.split('.')[0] for file_name in y_validation]\n",
    "y_test = [file_name.split('.')[0] for file_name in y_test]\n",
    "\n",
    "print(f\"Number of samples in training set: {len(X_train)}\")\n",
    "print(f\"Number of samples in validation set: {len(X_validation)}\")\n",
    "print(f\"Number of samples in test set: {len(X_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that defines a grid of hyperparameter values to search through in grid seach process\n",
    "param_grid = {\n",
    "    'neurons_per_layer': [32, 64, 128],\n",
    "    'num_hidden_layers': [1, 2, 3],\n",
    "    'activation': ['relu', 'sigmoid'],\n",
    "    'epochs': [10, 20, 30],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'momentum': [0.5, 0.9, 0.99]\n",
    "}\n",
    "\n",
    "# Initialize MLP object\n",
    "mlp = MLP(num_classes=15, image_width=320, image_height=243, num_channels=3)\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_result = mlp.tune(X_train, y_train, param_grid)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_result.best_estimator_.model\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "# Train the best model\n",
    "trained_model, history = mlp.fit(X_train, y_train, epochs=best_params['epochs'])\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = mlp.test(trained_model, X_test, y_test)\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "''' Create an instacne of keras classifier which allows keras based model with scikit learn.\n",
    "The build_fn is set to the create_model function which is used to create different model instances during\n",
    "the grid search with various hyperparameters. Verbose is set to 0(training progess wont be displayed)\n",
    "'''\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "'''Create an instance of the GridSearchCV class which performs a grid search over the specified hyperparamter grid\n",
    "It has the following parameters\n",
    "1) estimator: The model used in this case, The keras classifier model defined\n",
    "2) param_grid: The hyperparameters used in the search\n",
    "3) cv: Number of fold cross validation used for evaluating the models performance\n",
    "4) scoring: The metric used to evaluate the model\n",
    "5) n_jobs: -1 means that it will use all CPU cores for parallel computaion'''\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Start the Grid search by fitting the grid search object to the training data\n",
    "grid_result = grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_result.best_estimator_.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
